{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Loading and Processing of aData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\statsmodels\\tools\\_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature1</th>\n",
       "      <th>Feature2</th>\n",
       "      <th>Feature3</th>\n",
       "      <th>Feature4</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>CallCategory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A-</td>\n",
       "      <td>PreQ</td>\n",
       "      <td>M</td>\n",
       "      <td>SPR</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>PreQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A-</td>\n",
       "      <td>PreQ</td>\n",
       "      <td>M</td>\n",
       "      <td>SPR</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>PostQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AB+</td>\n",
       "      <td>PreQ</td>\n",
       "      <td>M</td>\n",
       "      <td>TMO</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>PreQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AB+</td>\n",
       "      <td>PreQ</td>\n",
       "      <td>M</td>\n",
       "      <td>TMO</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>PostQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AB+</td>\n",
       "      <td>PreQ</td>\n",
       "      <td>M</td>\n",
       "      <td>VER</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>PreQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332883</th>\n",
       "      <td>AB+</td>\n",
       "      <td>PostQ</td>\n",
       "      <td>I</td>\n",
       "      <td>TMO</td>\n",
       "      <td>Monday</td>\n",
       "      <td>PostQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332884</th>\n",
       "      <td>A+</td>\n",
       "      <td>PostQ</td>\n",
       "      <td>I</td>\n",
       "      <td>TMO</td>\n",
       "      <td>Monday</td>\n",
       "      <td>PreQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332885</th>\n",
       "      <td>AB+</td>\n",
       "      <td>PostQ</td>\n",
       "      <td>I</td>\n",
       "      <td>VER</td>\n",
       "      <td>Monday</td>\n",
       "      <td>PostQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332886</th>\n",
       "      <td>AB+</td>\n",
       "      <td>PostQ</td>\n",
       "      <td>I</td>\n",
       "      <td>SPR</td>\n",
       "      <td>Monday</td>\n",
       "      <td>PreQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332887</th>\n",
       "      <td>A+</td>\n",
       "      <td>PostQ</td>\n",
       "      <td>I</td>\n",
       "      <td>SPR</td>\n",
       "      <td>Monday</td>\n",
       "      <td>PostQ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>332888 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Feature1 Feature2 Feature3 Feature4 DayOfWeek CallCategory\n",
       "0            A-     PreQ        M      SPR  Saturday         PreQ\n",
       "1            A-     PreQ        M      SPR  Saturday        PostQ\n",
       "2           AB+     PreQ        M      TMO  Saturday         PreQ\n",
       "3           AB+     PreQ        M      TMO  Saturday        PostQ\n",
       "4           AB+     PreQ        M      VER  Saturday         PreQ\n",
       "...         ...      ...      ...      ...       ...          ...\n",
       "332883      AB+    PostQ        I      TMO    Monday        PostQ\n",
       "332884       A+    PostQ        I      TMO    Monday         PreQ\n",
       "332885      AB+    PostQ        I      VER    Monday        PostQ\n",
       "332886      AB+    PostQ        I      SPR    Monday         PreQ\n",
       "332887       A+    PostQ        I      SPR    Monday        PostQ\n",
       "\n",
       "[332888 rows x 6 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pgmpy.estimators import BDeuScore, K2Score, BicScore\n",
    "from pgmpy.models import BayesianModel\n",
    "\n",
    "data = pd.read_csv(\"HU_File_Q2_S1.csv\")\n",
    "data = data.drop(['Unnamed: 0', 'Calldate', 'DialStart','Calltime'], axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Learning Bayesian Networks\n",
    "\n",
    "\n",
    "Previous notebooks showed how Bayesian networks economically encode a probability distribution over a set of variables, and how they can be used e.g. to predict variable states, or to generate new samples from the joint distribution. This section will be about obtaining a Bayesian network, given a set of sample data. Learning a Bayesian network can be split into two problems:\n",
    "\n",
    " **Parameter learning:** Given a set of data samples and a DAG that captures the dependencies between the variables, estimate the (conditional) probability distributions of the individual variables.\n",
    " \n",
    " **Structure learning:** Given a set of data samples, estimate a DAG that captures the dependencies between the variables.\n",
    " \n",
    "This notebook aims to illustrate how parameter learning and structure learning can be done with pgmpy.\n",
    "Currently, the library supports:\n",
    " - Parameter learning for *discrete* nodes:\n",
    "   - Maximum Likelihood Estimation\n",
    "   - Bayesian Estimation\n",
    " - Structure learning for *discrete*, *fully observed* networks:\n",
    "   - Score-based structure estimation (BIC/BDeu/K2 score; exhaustive search, hill climb/tabu search)\n",
    "   - Constraint-based structure estimation (PC)\n",
    "   - Hybrid structure estimation (MMHC)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure Learning\n",
    "\n",
    "To learn model structure (a DAG) from a data set, there are two broad techniques:\n",
    "\n",
    " - score-based structure learning\n",
    " - constraint-based structure learning\n",
    "\n",
    "The combination of both techniques allows further improvement:\n",
    " - hybrid structure learning\n",
    "\n",
    "We briefly discuss all approaches and give examples.\n",
    "\n",
    "### Score-based Structure Learning\n",
    "\n",
    "\n",
    "This approach construes model selection as an optimization task. It has two building blocks:\n",
    "\n",
    "- A _scoring function_ $s_D\\colon M \\to \\mathbb R$ that maps models to a numerical score, based on how well they fit to a given data set $D$.\n",
    "- A _search strategy_ to traverse the search space of possible models $M$ and select a model with optimal score.\n",
    "\n",
    "\n",
    "#### Scoring functions\n",
    "\n",
    "Commonly used scores to measure the fit between model and data are _Bayesian Dirichlet scores_ such as *BDeu* or *K2* and the _Bayesian Information Criterion_ (BIC, also called MDL). See [1], Section 18.3 for a detailed introduction on scores. As before, BDeu is dependent on an equivalent sample size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the scores vary slightly, we can see that the correct `model1` has a much higher score than `model2`.\n",
    "Importantly, these scores _decompose_, i.e. they can be computed locally for each of the variables given their potential parents, independent of other parts of the network:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Search strategies\n",
    "The search space of DAGs is super-exponential in the number of variables and the above scoring functions allow for local maxima. The first property makes exhaustive search intractable for all but very small networks, the second prohibits efficient local optimization algorithms to always find the optimal structure. Thus, identifiying the ideal structure is often not tractable. Despite these bad news, heuristic search strategies often yields good results.\n",
    "\n",
    "`HillClimbSearch` implements a greedy local search that starts from the DAG `start` (default: disconnected DAG) and proceeds by iteratively performing single-edge manipulations that maximally increase the score. The search terminates once a local maximum is found.\n",
    "\n",
    "By Single-Edge manipulations, we refer to legal operations that can be performed on the network. Operations such as addition, deletion or modification of an edge between two nodes. \n",
    "\n",
    "\n",
    "We used BicScore as our Scoring functions. \n",
    "The BIC/MDL score (\"Bayesian Information Criterion\", also \"Minimal Descriptive Length\") is a\n",
    "log-likelihood score with an additional penalty for network complexity, to avoid overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Feature3', 'DayOfWeek')]\n"
     ]
    }
   ],
   "source": [
    "from pgmpy.estimators import HillClimbSearch\n",
    "\n",
    "hc = HillClimbSearch(data, scoring_method=K2Score(data))\n",
    "best_model = hc.estimate()\n",
    "# We used Hill Climbing to estimate the optimal Directed Acyclic Graph that best represents out model. \n",
    "print(best_model.edges()) \n",
    "## Output connected nodes/conditional dependecies, which we see that only one exists between DayOfWeek and Feature3. \n",
    "## As far as our output label is concerned: Call Category, it seems like its independent of any feature currently present in the dataset. \n",
    "## We also not that there are no dependencies between the features themselves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below we printed out the Leaves of our optimal DAG.\n",
      "In directed graphs, the common terminology is source for a node that has no incoming edges and sink for a node that has no outgoing edges. \n",
      "\n",
      "Leaves in optiomal DAG:  ['Feature1', 'Feature2', 'Feature4', 'DayOfWeek', 'CallCategory'] \n",
      "\n",
      "We note that other than Feature 3, every other feature including our output label-CallCategory- is present as a leaf. This means that there are niether edges orignating from these features nor are there any edges approaching them, signifying: \n",
      " \n",
      "1). There are no conditional dependencies between the features, Feature 3 being the exception. There is an outgoing edge from Feature 3 to DayOfWeek, which indicaties conditional dependency \n",
      "2). Our output label - CallCategory - is not dependent upon any feature or a combination of features\n",
      "\n",
      "\n",
      "To ensure point 2, we, specifically:\n",
      "\n",
      "A). Asked our program to check whether there are any local independencies of our CallCategory\n",
      "(CallCategory _|_ Feature1, DayOfWeek, Feature2, Feature4, Feature3) \n",
      "\n",
      "Ans). Call Category is independent of every feature.\n",
      "\n",
      "B). Asked our program to find all the observed variables which are d-connected to variables in our DAG when observed variables are observed.\n",
      "{'CallCategory': {'CallCategory'}} \n",
      "\n",
      "Ans). There were none\n",
      "C). Asked our program to check whether our output label has any ancestors, that is, is it conditionally dependent upon anything?\n",
      "{'CallCategory'} \n",
      "\n",
      "Ans). No. CallCategory is dependent just on itself.\n"
     ]
    }
   ],
   "source": [
    "print(\"Below we printed out the Leaves of our optimal DAG.\")\n",
    "print(\"In directed graphs, the common terminology is source for a node that has no incoming edges and sink for a node that has no outgoing edges. \\n\")\n",
    "\n",
    "print(\"Leaves in optiomal DAG: \", best_model.get_leaves(), \"\\n\")\n",
    "print(\"We note that other than Feature 3, every other feature including our output label-CallCategory- is present as a leaf. This means that there are niether edges orignating from these features nor are there any edges approaching them, signifying: \\n \")\n",
    "print(\"1). There are no conditional dependencies between the features, Feature 3 being the exception. There is an outgoing edge from Feature 3 to DayOfWeek, which indicaties conditional dependency \")\n",
    "print(\"2). Our output label - CallCategory - is not dependent upon any feature or a combination of features\\n\\n\")\n",
    "# print(\"Roots in optimal DAG: \",best_model.get_roots(), '\\n')\n",
    "\n",
    "print(\"To ensure point 2, we, specifically:\\n\")\n",
    "print(\"A). Asked our program to check whether there are any local independencies of our CallCategory\")\n",
    "print(best_model.local_independencies(\"CallCategory\"), '\\n')\n",
    "print(\"Ans). Call Category is independent of every feature.\\n\")\n",
    "\n",
    "print(\"B). Asked our program to find all the observed variables which are d-connected to variables in our DAG when observed variables are observed.\")\n",
    "print(best_model.active_trail_nodes(\"CallCategory\"), '\\n')\n",
    "print(\"Ans). There were none\")\n",
    "\n",
    "print(\"C). Asked our program to check whether our output label has any ancestors, that is, is it conditionally dependent upon anything?\")\n",
    "print(best_model._get_ancestors_of(\"CallCategory\"), '\\n')\n",
    "print(\"Ans). No. CallCategory is dependent just on itself.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Conditional) Independence Tests\n",
    "\n",
    "Independencies in the data can be identified using chi2 conditional independence tests. To this end, constraint-based estimators in pgmpy have a `test_conditional_independence(X, Y, Zs)`-method, that performs a hypothesis test on the data sample. It allows to check if `X` is independent from `Y` given a set of variables `Zs`.\n",
    "\n",
    "`test_conditional_independence()` returns a tripel `(chi2, p_value, sufficient_data)`, consisting in the computed chi2 test statistic, the `p_value` of the test, and a heuristig flag that indicates if the sample size was sufficient. The `p_value` is the probability of observing the computed chi2 statistic (or an even higher chi2 value), given the null hypothesis that X and Y are independent given Zs.\n",
    "\n",
    "This can be used to make independence judgements, at a given level of significance.\n",
    "\n",
    "We moved onto verify our DAG obtained by carrying out some Independent test (ch2 test) to see if our results match up with independencies in our DAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We carried out the Chi2 test on our output label with respect to every feature. If the function returns:\n",
      " 1). True - The two nodes are independent. \n",
      " 2). False - There is a dependency between the two nodes.\n",
      "\n",
      "\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "\n",
      "Our results do match up with our optimal DAG that we learnt through Hill Climbing. Chi2 tests reveal that the output label has no dependencies whatsoever. On the contrary, a dependency exists between Feature 3 and DayOfWeek.\n"
     ]
    }
   ],
   "source": [
    "from pgmpy.estimators import ConstraintBasedEstimator\n",
    "\n",
    " \n",
    "est = ConstraintBasedEstimator(data) #Fed the Data.\n",
    "# Returns result of hypothesis test for the null hypothesis that X _|_ Y | Zs, using a chi2 statistic and threshold `significance_level\n",
    "\n",
    "print(\"We carried out the Chi2 test on our output label with respect to every feature. If the function returns:\\n 1). True - The two nodes are independent. \\n 2). False - There is a dependency between the two nodes.\")\n",
    "print(\"\\n\")\n",
    "print(est.test_conditional_independence('CallCategory','Feature1'))          # independent\n",
    "print(est.test_conditional_independence('CallCategory','Feature2'))          # independent\n",
    "print(est.test_conditional_independence('CallCategory','Feature3'))          # independent\n",
    "print(est.test_conditional_independence('CallCategory','Feature4'))       # independent - True is independent\n",
    "print(est.test_conditional_independence('CallCategory','DayOfWeek'))      # independent\n",
    "print(est.test_conditional_independence('Feature3','DayOfWeek'))          # dependent - False is dependent\n",
    "\n",
    "print(\"\\nOur results do match up with our optimal DAG that we learnt through Hill Climbing. Chi2 tests reveal that the output label has no dependencies whatsoever. On the contrary, a dependency exists between Feature 3 and DayOfWeek.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Feature1 Feature2 Feature3 Feature4 DayOfWeek CallCategory\n",
      "0            A-     PreQ        M      SPR  Saturday         PreQ\n",
      "1            A-     PreQ        M      SPR  Saturday        PostQ\n",
      "2           AB+     PreQ        M      TMO  Saturday         PreQ\n",
      "3           AB+     PreQ        M      TMO  Saturday        PostQ\n",
      "4           AB+     PreQ        M      VER  Saturday         PreQ\n",
      "...         ...      ...      ...      ...       ...          ...\n",
      "332883      AB+    PostQ        I      TMO    Monday        PostQ\n",
      "332884       A+    PostQ        I      TMO    Monday         PreQ\n",
      "332885      AB+    PostQ        I      VER    Monday        PostQ\n",
      "332886      AB+    PostQ        I      SPR    Monday         PreQ\n",
      "332887       A+    PostQ        I      SPR    Monday        PostQ\n",
      "\n",
      "[332888 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that the variables relate as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We constructed our optimal DAG manually that we learnt above. This has been done in order to learn the parameters now.\n"
     ]
    }
   ],
   "source": [
    "from pgmpy.models import BayesianModel\n",
    "# model = BayesianModel([('Feature1'), ('Feature2'),('Feature3',\"DayOfWeek\"),('Feature4'),('CallCategory')])  # fruit -> tasty <- size# model = best_model\n",
    "print(\"We constructed our optimal DAG manually that we learnt above. This has been done in order to learn the parameters now.\")\n",
    "##We constructed our optimal DAG manually that we learnt above. This has been done in order to learn the parameters now.\n",
    "\n",
    "model = BayesianModel() \n",
    "model.add_node(\"Feature1\")\n",
    "model.add_node(\"Feature2\")\n",
    "model.add_node(\"Feature3\")\n",
    "model.add_node(\"Feature4\")\n",
    "model.add_node(\"DayOfWeek\")\n",
    "model.add_node(\"CallCategory\")\n",
    "model.add_edge(\"Feature3\",\"DayOfWeek\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter learning is the task to estimate the values of the conditional probability distributions (CPDs), for the variables `fruit`, `size`, and `tasty`. \n",
    "\n",
    "#### State counts\n",
    "To make sense of the given data, we can start by counting how often each state of the variable occurs. If the variable is dependent on parents, the counts are done conditionally on the parents states, i.e. for seperately for each parent configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        CallCategory\n",
      "PostQ        166444\n",
      "PreQ         166444\n"
     ]
    }
   ],
   "source": [
    "from pgmpy.estimators import ParameterEstimator\n",
    "pe = ParameterEstimator(model, data)\n",
    "print(\"\\n\", pe.state_counts('CallCategory'))  # unconditional\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see, for example, that as many apples as bananas were observed and that `5` large bananas were tasty, while only `1` was not.\n",
    "\n",
    "#### Maximum Likelihood Estimation\n",
    "\n",
    "A natural estimate for the CPDs is to simply use the *relative frequencies*, with which the variable states have occured. We observed `7 apples` among a total of `14 fruits`, so we might guess that about `50%` of `fruits` are `apples`.\n",
    "\n",
    "This approach is *Maximum Likelihood Estimation (MLE)*. According to MLE, we should fill the CPDs in such a way, that $P(\\text{data}|\\text{model})$ is maximal. This is achieved when using the *relative frequencies*. See [1], section 17.1 for an introduction to ML parameter estimation. pgmpy supports MLE as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Learnt the parameters from the data. At any given point in time, there is a 50% chance that the CallCategory could either be PostQ or PreQ. This chance is also bot affected by any other features since CallCategory is completely independent.\n",
      "+---------------------+-----+\n",
      "| CallCategory(PostQ) | 0.5 |\n",
      "+---------------------+-----+\n",
      "| CallCategory(PreQ)  | 0.5 |\n",
      "+---------------------+-----+\n",
      "+---------------+----------+\n",
      "| Feature1(A+)  | 0.142063 |\n",
      "+---------------+----------+\n",
      "| Feature1(A-)  | 0.285841 |\n",
      "+---------------+----------+\n",
      "| Feature1(AB+) | 0.428357 |\n",
      "+---------------+----------+\n",
      "| Feature1(AB-) | 0.143739 |\n",
      "+---------------+----------+\n",
      "+-----------------+----------+\n",
      "| Feature2(PostQ) | 0.507065 |\n",
      "+-----------------+----------+\n",
      "| Feature2(PreQ)  | 0.492935 |\n",
      "+-----------------+----------+\n",
      "+-------------+----------+\n",
      "| Feature3(F) | 0.291573 |\n",
      "+-------------+----------+\n",
      "| Feature3(I) | 0.303369 |\n",
      "+-------------+----------+\n",
      "| Feature3(M) | 0.405058 |\n",
      "+-------------+----------+\n",
      "+---------------+----------+\n",
      "| Feature4(SPR) | 0.370019 |\n",
      "+---------------+----------+\n",
      "| Feature4(TMO) | 0.44436  |\n",
      "+---------------+----------+\n",
      "| Feature4(VER) | 0.185621 |\n",
      "+---------------+----------+\n"
     ]
    }
   ],
   "source": [
    "from pgmpy.estimators import MaximumLikelihoodEstimator\n",
    "mle = MaximumLikelihoodEstimator(model, data)\n",
    "\n",
    "print(\"\\nLearnt the parameters from the data. At any given point in time, there is a 50% chance that the CallCategory could either be PostQ or PreQ. This chance is also bot affected by any other features since CallCategory is completely independent.\")\n",
    "print(mle.estimate_cpd('CallCategory'))  \n",
    "print(mle.estimate_cpd('Feature1'))  \n",
    "print(mle.estimate_cpd('Feature2'))  \n",
    "print(mle.estimate_cpd('Feature3'))  \n",
    "print(mle.estimate_cpd('Feature4')) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`mle.estimate_cpd(variable)` computes the state counts and divides each cell by the (conditional) sample size. The `mle.get_parameters()`-method returns a list of CPDs for all variable of the model.\n",
    "\n",
    "The built-in `fit()`-method of `BayesianModel` provides more convenient access to parameter estimators:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibrate all CPDs of `model` using MLE:\n",
    "model.fit(data, estimator=MaximumLikelihoodEstimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "While very straightforward, the ML estimator has the problem of *overfitting* to the data. In above CPD, the probability of a large banana being tasty is estimated at `0.833`, because `5` out of `6` observed large bananas were tasty. Fine. But note that the probability of a small banana being tasty is estimated at `0.0`, because we  observed only one small banana and it happened to be not tasty. But that should hardly make us certain that small bananas aren't tasty!\n",
    "We simply do not have enough observations to rely on the observed frequencies. If the observed data is not representative for the underlying distribution, ML estimations will be extremly far off. \n",
    "\n",
    "When estimating parameters for Bayesian networks, lack of data is a frequent problem. Even if the total sample size is very large, the fact that state counts are done conditionally for each parents configuration causes immense fragmentation. If a variable has 3 parents that can each take 10 states, then state counts will be done seperately for `10^3 = 1000` parents configurations. This makes MLE very fragile and unstable for learning Bayesian Network parameters. A way to mitigate MLE's overfitting is *Bayesian Parameter Estimation*.\n",
    "\n",
    "#### Bayesian Parameter Estimation\n",
    "\n",
    "The Bayesian Parameter Estimator starts with already existing prior CPDs, that express our beliefs about the variables *before* the data was observed. Those \"priors\" are then updated, using the state counts from the observed data. See [1], Section 17.3 for a general introduction to Bayesian estimators.\n",
    "\n",
    "One can think of the priors as consisting in *pseudo state counts*, that are added to the actual counts before normalization.\n",
    "Unless one wants to encode specific beliefs about the distributions of the variables, one commonly chooses uniform priors, i.e. ones that deem all states equiprobable.\n",
    "\n",
    "A very simple prior is the so-called *K2* prior, which simply adds `1` to the count of every single state.\n",
    "A somewhat more sensible choice of prior is *BDeu* (Bayesian Dirichlet equivalent uniform prior). For BDeu we need to specify an *equivalent sample size* `N` and then the pseudo-counts are the equivalent of having observed `N` uniform samples of each variable (and each parent configuration). In pgmpy:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-----+\n",
      "| CallCategory(PostQ) | 0.5 |\n",
      "+---------------------+-----+\n",
      "| CallCategory(PreQ)  | 0.5 |\n",
      "+---------------------+-----+\n"
     ]
    }
   ],
   "source": [
    "from pgmpy.estimators import BayesianEstimator\n",
    "est = BayesianEstimator(model, data)\n",
    "\n",
    "print(est.estimate_cpd('CallCategory', prior_type='BDeu', equivalent_sample_size=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimated values in the CPDs are now more conservative. In particular, the estimate for a small banana being not tasty is now around `0.64` rather than `1.0`. Setting `equivalent_sample_size` to `10` means that for each parent configuration, we add the equivalent of 10 uniform samples (here: `+5` small bananas that are tasty and `+5` that aren't).\n",
    "\n",
    "`BayesianEstimator`, too, can be used via the `fit()`-method. Full example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Replacing existing CPD for Feature1\n",
      "WARNING:root:Replacing existing CPD for Feature2\n",
      "WARNING:root:Replacing existing CPD for Feature3\n",
      "WARNING:root:Replacing existing CPD for Feature4\n",
      "WARNING:root:Replacing existing CPD for DayOfWeek\n",
      "WARNING:root:Replacing existing CPD for CallCategory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-----+\n",
      "| CallCategory(PostQ) | 0.5 |\n",
      "+---------------------+-----+\n",
      "| CallCategory(PreQ)  | 0.5 |\n",
      "+---------------------+-----+\n",
      "+----------------------+---------------------+---------------------+---------------------+\n",
      "| Feature3             | Feature3(F)         | Feature3(I)         | Feature3(M)         |\n",
      "+----------------------+---------------------+---------------------+---------------------+\n",
      "| DayOfWeek(Friday)    | 0.1463615062630132  | 0.1439279737719512  | 0.13909185232376556 |\n",
      "+----------------------+---------------------+---------------------+---------------------+\n",
      "| DayOfWeek(Monday)    | 0.2136582355238344  | 0.21807417354816597 | 0.237000000706301   |\n",
      "+----------------------+---------------------+---------------------+---------------------+\n",
      "| DayOfWeek(Saturday)  | 0.08506090321618436 | 0.08467438677130099 | 0.07245023579858274 |\n",
      "+----------------------+---------------------+---------------------+---------------------+\n",
      "| DayOfWeek(Sunday)    | 0.06932879887122508 | 0.06708842913207057 | 0.05623109320559645 |\n",
      "+----------------------+---------------------+---------------------+---------------------+\n",
      "| DayOfWeek(Thursday)  | 0.14844263597989715 | 0.147106516791204   | 0.14521018457763554 |\n",
      "+----------------------+---------------------+---------------------+---------------------+\n",
      "| DayOfWeek(Tuesday)   | 0.17934020043997106 | 0.18150607582199596 | 0.19195424299720235 |\n",
      "+----------------------+---------------------+---------------------+---------------------+\n",
      "| DayOfWeek(Wednesday) | 0.15780771970587487 | 0.1576224441633114  | 0.1580623903909164  |\n",
      "+----------------------+---------------------+---------------------+---------------------+\n",
      "+---------------+----------+\n",
      "| Feature1(A+)  | 0.142064 |\n",
      "+---------------+----------+\n",
      "| Feature1(A-)  | 0.28584  |\n",
      "+---------------+----------+\n",
      "| Feature1(AB+) | 0.428355 |\n",
      "+---------------+----------+\n",
      "| Feature1(AB-) | 0.143741 |\n",
      "+---------------+----------+\n",
      "+-----------------+----------+\n",
      "| Feature2(PostQ) | 0.507065 |\n",
      "+-----------------+----------+\n",
      "| Feature2(PreQ)  | 0.492935 |\n",
      "+-----------------+----------+\n",
      "+-------------+----------+\n",
      "| Feature3(F) | 0.291573 |\n",
      "+-------------+----------+\n",
      "| Feature3(I) | 0.30337  |\n",
      "+-------------+----------+\n",
      "| Feature3(M) | 0.405057 |\n",
      "+-------------+----------+\n",
      "+---------------+----------+\n",
      "| Feature4(SPR) | 0.370019 |\n",
      "+---------------+----------+\n",
      "| Feature4(TMO) | 0.444358 |\n",
      "+---------------+----------+\n",
      "| Feature4(VER) | 0.185623 |\n",
      "+---------------+----------+\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pgmpy.models import BayesianModel\n",
    "from pgmpy.estimators import BayesianEstimator\n",
    "\n",
    "# generate data\n",
    "model.fit(data, estimator=BayesianEstimator, prior_type=\"BDeu\") # default equivalent_sample_size=5\n",
    "for cpd in model.get_cpds():\n",
    "    print(cpd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "[1] http://pgmpy.org/index.html\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
